{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook is summary of the experiment results ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two way to view tensorboard:\n",
    "\n",
    "1. To start a TensorBoard session from VSC:\n",
    "\n",
    "    1. Select the Python Interpreter for the notebook. You can select python interpreter from Command Palette -> Python Select Interpreter\n",
    "    2. Open the command palette (Ctrl/Cmd + Shift + P)\n",
    "    3. Search for the command “Python: Launch TensorBoard” and press enter.\n",
    "    4. You will be able to select the folder where your TensorBoard log files are located. By default, the current working directory will be used.\n",
    "\n",
    "  VSCode will then open a new tab with TensorBoard and its lifecycle will be managed by VS Code as well.\n",
    "\n",
    "2. From command prompt: \n",
    "    \n",
    "    1. Open terminal\n",
    "\n",
    "        > log_path = 'logs' #Based on the folder in the terminal this will change. I was always on parent dir so provide relative path</br>\n",
    "\n",
    "        > tensorboard --logdir log_path --host localhost --port 8888\n",
    "        \n",
    "            OR\n",
    "        \n",
    "        > tensorboard --logdir 'logs' --host localhost --port 8888\n",
    "\n",
    "    2. Open web browser and type in URL\n",
    "        http//localhost:8888\n",
    "\n",
    "Note:\n",
    "1. Move up to directory \n",
    "     log_path = os.path.join(os.getcwd() ,\"..\\\\logs\")#Since in my case to reach log folder have to move 1 up to reach parent director. So used single '..\\\\' \n",
    "2. https://stackoverflow.com/questions/63938552/how-to-run-tensorboard-in-vscode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List of selected hyperparam with gave decent F1-score result**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>test name</td>\n",
    "        <td>lr</td>\n",
    "        <td>lstm unit</td>\n",
    "        <td>dropout</td>\n",
    "        <td>recurent dropout</td>\n",
    "        <td>l2</td>\n",
    "        <td>batch size</td>\n",
    "        <td>epoch</td>\n",
    "        <td>f1-score </td>\n",
    "        <td></td>\n",
    "        <td>remark </td>\n",
    "    </tr>\n",
    "    <tr>   \n",
    "        <td> </td>\n",
    "        <td> </td>\n",
    "        <td> </td>\n",
    "        <td> </td>\n",
    "        <td> </td>\n",
    "        <td> </td>\n",
    "        <td> </td>\n",
    "        <td> </td>\n",
    "        <td>train</td>\n",
    "        <td>valid</td>\n",
    "        <td> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Bidirection LSTM</td>\n",
    "        <td>0.01</td>\n",
    "        <td>256</td>\n",
    "        <td>0.2</td>\n",
    "        <td>0.2</td>\n",
    "        <td>default</td>\n",
    "        <td>2048</td>\n",
    "        <td>150</td>\n",
    "        <td>0.48</td>\n",
    "        <td>0.42</td>\n",
    "        <td>Overfitting. Best value train: 0.43, Valid:0.425, Epoch:85 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Trainable Weighted bi-direction LSTM</td>\n",
    "        <td>0.002</td>\n",
    "        <td>256</td>\n",
    "        <td>0.2</td>\n",
    "        <td>0.2</td>\n",
    "        <td>default</td>\n",
    "        <td>512</td>\n",
    "        <td>41</td>\n",
    "        <td>0.61</td>\n",
    "        <td>0.49</td>\n",
    "        <td>Overfitting. Best value train: 0.4165, Valid:0.4001, Epoch:20 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Trainable bi-direction LSTM</td>\n",
    "        <td>0.002</td>\n",
    "        <td>256</td>\n",
    "        <td>0.2</td>\n",
    "        <td>0.2</td>\n",
    "        <td>default</td>\n",
    "        <td>512</td>\n",
    "        <td>25</td>\n",
    "        <td>0.56</td>\n",
    "        <td>0.48</td>\n",
    "        <td>Overfitting. Best value train: 0.3467, Valid:0.3392, Epoch:16 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Weighted LSTM</td>\n",
    "        <td>0.002</td>\n",
    "        <td>128</td>\n",
    "        <td>0.2</td>\n",
    "        <td>0.1</td>\n",
    "        <td>0.0003</td>\n",
    "        <td>256</td>\n",
    "        <td>300</td>\n",
    "        <td>0.5901</td>\n",
    "        <td>0.4816</td>\n",
    "        <td>Overfitting. Best value train: 0.4462, Valid:0.4319, Epoch:115 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Bert</td>\n",
    "        <td>1.00E-05</td>\n",
    "        <td>256</td>\n",
    "        <td>0.1</td>\n",
    "        <td>-</td>\n",
    "        <td>-</td>\n",
    "        <td>19*8 (TPU)</td>\n",
    "        <td>250</td>\n",
    "        <td>0.5477</td>\n",
    "        <td>0.5098</td>\n",
    "        <td>Tensor size issue so kept the batch size smaller. Overfitting. Best value train: 0.5088, Valid:0.5022, Epoch:16 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Bert</td>\n",
    "        <td>1.00E-05</td>\n",
    "        <td>256</td>\n",
    "        <td>0.2</td>\n",
    "        <td>-</td>\n",
    "        <td>-</td>\n",
    "        <td>64*8 (TPU)</td>\n",
    "        <td>16</td>\n",
    "        <td>0.2646</td>\n",
    "        <td>0.3072</td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Bert</td>\n",
    "        <td>1.00E-05</td>\n",
    "        <td>256</td>\n",
    "        <td>0.2</td>\n",
    "        <td>-</td>\n",
    "        <td>-</td>\n",
    "        <td>64*8 (TPU)</td>\n",
    "        <td>32</td>\n",
    "        <td>0.4973</td>\n",
    "        <td>0.4995</td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Bert</td>\n",
    "        <td>1.00E-05</td>\n",
    "        <td>256</td>\n",
    "        <td>0.2</td>\n",
    "        <td>-</td>\n",
    "        <td>-</td>\n",
    "        <td>64*8 (TPU)</td>\n",
    "        <td>50</td>\n",
    "        <td>0.6795</td>\n",
    "        <td>0.508</td>\n",
    "        <td>Overfitting on training</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Bert</td>\n",
    "        <td>1.00E-05</td>\n",
    "        <td>256</td>\n",
    "        <td>0.2</td>\n",
    "        <td>-</td>\n",
    "        <td>-</td>\n",
    "        <td>20*8 (TPU)</td>\n",
    "        <td>16</td>\n",
    "        <td>0.5261</td>\n",
    "        <td>0.5078</td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Bert</td>\n",
    "        <td>1.00E-05</td>\n",
    "        <td>256</td>\n",
    "        <td>0.2</td>\n",
    "        <td>-</td>\n",
    "        <td>-</td>\n",
    "        <td>19*8 (TPU)</td>\n",
    "        <td>15</td>        \n",
    "        <td>0.5044</td>\n",
    "        <td>0.5039</td>\n",
    "        <td>Reasonable. Will use this model for Evaluating</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Bert</td>\n",
    "        <td>4.00E-05</td>\n",
    "        <td>210</td>\n",
    "        <td>0.2</td>\n",
    "        <td>-</td>\n",
    "        <td>-</td>\n",
    "        <td>64*8 (TPU)</td>\n",
    "        <td>50</td>\n",
    "        <td>0.7637</td>\n",
    "        <td>0.5012</td>\n",
    "        <td>Overfitting.  Best value train: 0.5234, Valid:0.5049, Epoch:11 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval the model on the test dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.path_utils import get_project_root\n",
    "\n",
    "#from pathlib import Path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "#import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-06 16:24:48,173: INFO: common: yaml file: F:\\github_workspace\\stackoverflow_tag_question\\config\\config.yaml loaded successfully]\n",
      "[2024-01-06 16:24:48,308: INFO: common: yaml file: F:\\github_workspace\\stackoverflow_tag_question\\params.yaml loaded successfully]\n",
      "[2024-01-06 16:24:48,341: INFO: common: yaml file: F:\\github_workspace\\stackoverflow_tag_question\\schema.yaml loaded successfully]\n",
      "\n",
      "DataIngestionConfig(artifacts_dir='artifacts', root_dir='data_ingestion', data_downlad_url='https://www.kaggle.com/competitions/stackoverflow-moderation/data', zip_data='stackoverflow-moderation.zip', data_filename='SO_Tag_prediction_test.parquet', data_test_filename='test.parquet')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(WindowsPath('F:/github_workspace/stackoverflow_tag_question/artifacts/data_ingestion/SO_Tag_prediction_test.parquet'),\n",
       " WindowsPath('F:/github_workspace/stackoverflow_tag_question/artifacts/data_ingestion/test.parquet'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.config.configuration import ConfigurationManager\n",
    "from src.entity.data_ingestion import DataIngestionConfig\n",
    "from src.entity.data_cleaning import DataCleaningConfig\n",
    "\n",
    "config = ConfigurationManager() #ConfigurationManager(notebook_artifacts = True)\n",
    "\n",
    "model_train_artifact_config = config.get_model_artifacts_config()\n",
    "data_ingestion_config = config.get_data_ingestion_config()\n",
    "data_cleaning_config = config.get_data_cleaning_config()\n",
    "\n",
    "sample_test_filename = 'SO_Tag_prediction_test.parquet'\n",
    "sample_data_save_path = get_project_root().joinpath(data_ingestion_config.artifacts_dir).joinpath(data_ingestion_config.root_dir).joinpath(sample_test_filename)\n",
    "\n",
    "#Sample from the test file\n",
    "test_filename = get_project_root().joinpath(data_ingestion_config.artifacts_dir).joinpath(data_ingestion_config.root_dir).joinpath(data_ingestion_config.data_test_filename)\n",
    "\n",
    "data_ingestion_config = DataIngestionConfig(artifacts_dir = data_ingestion_config.artifacts_dir,\n",
    "                                            root_dir = data_ingestion_config.root_dir,  \n",
    "                                            data_downlad_url = data_ingestion_config.data_downlad_url,\n",
    "                                            zip_data = data_ingestion_config.zip_data,\n",
    "                                            data_filename = sample_test_filename,\n",
    "                                            data_test_filename = data_ingestion_config.data_test_filename\n",
    ")\n",
    "\n",
    "data_cleaning_config = DataCleaningConfig(artifacts_dir = data_cleaning_config.artifacts_dir,\n",
    "                                          root_dir = data_cleaning_config.root_dir,\n",
    "                                          selected_col = data_cleaning_config.selected_col,\n",
    "                                          tag_col = data_cleaning_config.tag_col,\n",
    "                                          cleaned_data_filename = 'test_tags_text_processed.parquet'\n",
    ")\n",
    "\n",
    "#del [data_cleaning_config, data_ingestion_config]\n",
    "\n",
    "print()\n",
    "print(data_ingestion_config)\n",
    "(sample_data_save_path, test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample and Clean test dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbset = pd.read_parquet(test_filename)\n",
    "dbset = dbset.sample(100, random_state = 42)\n",
    "dbset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbset['BodyMarkdown'] = \"\"\n",
    "dbset[['Tag1', 'Tag2', 'Tag3', 'Tag4','Tag5']]  = dbset.tags.str.split(' ', expand = True) #.apply(lambda x: x.split()[:5])\n",
    "dbset.fillna(\"\", inplace = True)\n",
    "\n",
    "dbset.rename(columns = {'question':'Title'}, inplace = True)\n",
    "dbset.drop(columns = ['tags'], inplace = True)\n",
    "dbset.head()\n",
    "\n",
    "dbset.to_parquet(sample_data_save_path, engine = 'fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.entity.data_validation import DataValidation\n",
    "\n",
    "data_validation_config = config.get_data_validation_config()\n",
    "data_validation_obj = DataValidation(Config = data_validation_config, \n",
    "                                     Data_Config = data_ingestion_config,\n",
    "                                    )\n",
    "\n",
    "data_validation_obj.validate_all_columns()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.entity.data_cleaning import DataCleaning\n",
    "\n",
    "data_cleaning_obj = DataCleaning(Config = data_cleaning_config, \n",
    "                                 Data_Config = data_ingestion_config,\n",
    "                                )\n",
    "\n",
    "data_cleaning_obj.cleaning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Validate on test data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('F:/github_workspace/stackoverflow_tag_question/artifacts/data_cleaning/test_tags_text_processed.parquet')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_filename = get_project_root().joinpath(data_cleaning_config.artifacts_dir).joinpath(data_cleaning_config.root_dir).joinpath(data_cleaning_config.cleaned_data_filename)\n",
    "\n",
    "test_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>javascript,asp.net,textbox</td>\n",
       "      <td>add color palett aspnet textbox add color pale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jquery,jquery-ui,slider,jquery-animate</td>\n",
       "      <td>anim slider handl alter jqueri ui slider valu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uiscrollview</td>\n",
       "      <td>algorithm calcul uiscrollview landscap content...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>titanium,appcelerator,getjson,appcelerator-mob...</td>\n",
       "      <td>refresh dont append data refresh dont append d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>javascript,raphael</td>\n",
       "      <td>attach bind text element raphael attach bind t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tags  \\\n",
       "0                         javascript,asp.net,textbox   \n",
       "1             jquery,jquery-ui,slider,jquery-animate   \n",
       "2                                       uiscrollview   \n",
       "3  titanium,appcelerator,getjson,appcelerator-mob...   \n",
       "4                                 javascript,raphael   \n",
       "\n",
       "                                               title  \n",
       "0  add color palett aspnet textbox add color pale...  \n",
       "1  anim slider handl alter jqueri ui slider valu ...  \n",
       "2  algorithm calcul uiscrollview landscap content...  \n",
       "3  refresh dont append data refresh dont append d...  \n",
       "4  attach bind text element raphael attach bind t...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbset = pd.read_parquet(test_filename)\n",
    "dbset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps: Prepare data ####\n",
    "\n",
    "1. Convert comma seperated tags into onehot encoded columns as Y labels\n",
    "2. Take selected features name from Y labels\n",
    "3. Text tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\github_workspace\\stackoverflow_tag_question\\.env\\lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator MultiLabelBinarizer from version 1.2.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "foldername = \"train_bert\"\n",
    "serialize_objects_dir = get_project_root().joinpath(model_train_artifact_config.artifacts_dir).joinpath(model_train_artifact_config.serialize_objects_dir).joinpath(foldername)\n",
    "serialize_multilabel_binarizer_obj_dir = serialize_objects_dir.joinpath(\"multilabel_binarizer_obj.pickle\")\n",
    "#serialize_multilabel_binarizer_obj_dir\n",
    "\n",
    "#Step 1: Load object to convert the comma seperated tags into Y labels\n",
    "\n",
    "#Check if the file not found throw exception\n",
    "if serialize_multilabel_binarizer_obj_dir.exists():\n",
    "\n",
    "       #Load multi label binarizer object\n",
    "       with open(serialize_multilabel_binarizer_obj_dir,'rb') as file:\n",
    "              multilabel_binarizer_obj = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y label shape: (100, 14878)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\github_workspace\\stackoverflow_tag_question\\.env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:900: UserWarning: unknown class(es) ['android-loadermanager', 'approximation-hardness', 'candidate', 'cc.complexity-theory', 'composite-component', 'dynamical-systems', 'fastreport', 'function-calls', 'google-api-python-client', 'hcard', 'icacls', 'jquery-animate', 'mpm-prefork', 'retina-display', 'sharepoint-enterprise', 'tortoise-svn', 'window-manager', 'windows-search', 'word-processing'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert the tags\n",
    "Y = multilabel_binarizer_obj.transform(dbset.tags.apply(lambda x: { i for i in x.split(',')}))\n",
    "\n",
    "print(f'Y label shape: {Y.shape}')\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   13,   163,   186,   193,   194,   223,   225,   246,   315,\n",
       "          342,   374,   464,   483,   520,   598,   654,   681,   682,\n",
       "          703,   727,   783,   797,   836,   846,   880,   964,   999,\n",
       "         1047,  1095,  1107,  1171,  1211,  1259,  1260,  1373,  1374,\n",
       "         1484,  1501,  1608,  1609,  1611,  1658,  1679,  1699,  1829,\n",
       "         1929,  2035,  2091,  2253,  2268,  2645,  2745,  2758,  2917,\n",
       "         2930,  2938,  2977,  3092,  3171,  3180,  3219,  3438,  3645,\n",
       "         3724,  3800,  3966,  4060,  4130,  4134,  4143,  4148,  4149,\n",
       "         4153,  4157,  4226,  4251,  4331,  4452,  4501,  4703,  4889,\n",
       "         4898,  4969,  5018,  5066,  5167,  5213,  5223,  5236,  5413,\n",
       "         5503,  5748,  5774,  5790,  5794,  5996,  6016,  6368,  6375,\n",
       "         6399,  6548,  6599,  6605,  6624,  6782,  6820,  6833,  6862,\n",
       "         6864,  6884,  6989,  7066,  7174,  7319,  7334,  7478,  7496,\n",
       "         7521,  7828,  7925,  7947,  8081,  8386,  8392,  8412,  8700,\n",
       "         8738,  8758,  8916,  8961,  9133,  9193,  9298,  9362,  9406,\n",
       "         9508,  9511,  9554,  9625,  9642,  9807,  9842,  9945, 10212,\n",
       "        10318, 10427, 10517, 10523, 10534, 10587, 10650, 10730, 10733,\n",
       "        10930, 11120, 11127, 11131, 11229, 11263, 11279, 11332, 11362,\n",
       "        11411, 11623, 11844, 11938, 12202, 12293, 12324, 12386, 12388,\n",
       "        12410, 12584, 12597, 12664, 12794, 12868, 12924, 12928, 13067,\n",
       "        13068, 13260, 13434, 13589, 13592, 13757, 13764, 13766, 13775,\n",
       "        13837, 13838, 13849, 13884, 13988, 14055, 14264, 14312, 14316,\n",
       "        14360, 14364, 14372, 14396, 14460, 14504, 14580, 14605, 14778,\n",
       "        14788, 14789], dtype=int64),)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if all rows of column is empty/null/zero\n",
    "arr = Y.sum(axis = 0)\n",
    "non_zero_idx = arr.nonzero()\n",
    "non_zero_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_zero_idx[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "With this test dataset outof 14878 label only 209 label are associate to some records. Other label are all zeros i.e there are no records belong to those label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 1, 5, 2, 2, 2, 2, 4, 3, 3, 5, 2, 5, 5, 4, 1, 2, 1, 3, 5, 1,\n",
       "       4, 2, 4, 4, 3, 2, 3, 3, 5, 3, 2, 3, 3, 0, 3, 2, 1, 2, 4, 3, 2, 1,\n",
       "       4, 4, 1, 2, 3, 3, 5, 0, 4, 5, 4, 5, 1, 2, 3, 3, 2, 2, 3, 3, 4, 5,\n",
       "       2, 3, 4, 1, 3, 2, 4, 2, 3, 3, 3, 2, 3, 3, 4, 4, 3, 4, 1, 2, 1, 3,\n",
       "       3, 3, 1, 1, 5, 4, 4, 3, 5, 4, 4, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if each records have atleast 1 label assigned to it\n",
    "Y.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observartion:**\n",
    "\n",
    "There are records where does not have any label. We shall drop those records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Load list of selected features name\n",
    "serialize_selected_features_names_dir = serialize_objects_dir.joinpath(\"selected_features_names.pickle\")\n",
    "\n",
    "#Check if the file not found throw exception\n",
    "if serialize_selected_features_names_dir.exists():\n",
    "\n",
    "       #Open and read the file\n",
    "       with open(serialize_selected_features_names_dir,'rb') as file:\n",
    "              selected_features_name = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'php'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_features_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14878"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(multilabel_binarizer_obj.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 328 ms\n",
      "Wall time: 411 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#multilabel_binarizer_obj.classes_ is array object. \n",
    "def find_indices(list_to_check, item_to_find):\n",
    "    return [idx for idx, value in enumerate(list_to_check) if value == item_to_find ] # Option 2. This take  4.5 ms\n",
    "    #return np.where(multilabel_binarizer_obj.classes_ == item_to_find) # Option 1: This take 171ms\n",
    "\n",
    "selected_tags_idx = [ find_indices(multilabel_binarizer_obj.classes_, item_to_find)[0] for item_to_find in selected_features_name ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 185)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take the selected columns\n",
    "Y_test = Y[:,selected_tags_idx]\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 3: Removing the question with no tags selected\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "row_idx = np.nonzero(Y_test.sum(axis = 1))[0] #Question with nonzero tags\n",
    "Y_test = Y_test[row_idx,:]\n",
    "X_test = dbset.iloc[row_idx,:]\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del dbset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\github_workspace\\stackoverflow_tag_question\\.env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-06 16:28:40,361: INFO: common: yaml file: F:\\github_workspace\\stackoverflow_tag_question\\config\\config.yaml loaded successfully]\n",
      "[2024-01-06 16:28:40,377: INFO: common: yaml file: F:\\github_workspace\\stackoverflow_tag_question\\params.yaml loaded successfully]\n",
      "[2024-01-06 16:28:40,384: INFO: common: yaml file: F:\\github_workspace\\stackoverflow_tag_question\\schema.yaml loaded successfully]\n"
     ]
    }
   ],
   "source": [
    "from src.train_scripts.model_bert import model_hyperparam, model_tokenizer, model_import_tokenizer\n",
    "\n",
    "## Step 4: Int tokenizer on text \n",
    "\n",
    "#Load the tokenizer\n",
    "#MODEL_NAME = 'bert-large-uncased'\n",
    "#hyper_param = {}\n",
    "#hyper_param['model_name'] = MODEL_NAME\n",
    "hyper_param = model_hyperparam()\n",
    "\n",
    "tokenizer = model_tokenizer(hyper_param) \n",
    "\n",
    "#serialize_tokenizer_obj_dir = serialize_objects_dir.joinpath(\"tokenizer_obj.pickle\")\n",
    "#tokenizer = model_import_tokenizer(serialize_tokenizer_obj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_scripts.distributed_train import define_distributed_strategy\n",
    "\n",
    "strategy, num_replicas_in_sync = define_distributed_strategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec={'input_ids': TensorSpec(shape=(None, 80), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, 80), dtype=tf.int64, name=None)}>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 7: Create the dataset\n",
    "from src.train_scripts.model_bert import model_encode\n",
    "import tensorflow as tf\n",
    "\n",
    "test_batch = tf.data.Dataset.from_tensor_slices(X_test).map(lambda x: model_encode(x, tokenizer, hyper_param['max_title_len']), num_parallel_calls = tf.data.experimental.AUTOTUNE).prefetch(tf.data.AUTOTUNE) \n",
    "\n",
    "test_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps: Load model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_scripts.model_bert import  define_model\n",
    "\n",
    "with strategy.scope(): \n",
    "\n",
    "    model = define_model(hyper_param)\n",
    "    model.compile()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 8: Load the model\n",
    "\n",
    "model_artifacts = config.get_model_artifacts_config()\n",
    "checkpoint = \"logs_hparam_lr1e-05_units256_dropout02_batch19_epoch15\" \n",
    "checkpoint_dir = get_project_root().joinpath(model_artifacts.artifacts_dir).joinpath(model_artifacts.checkpoint_dir).joinpath(checkpoint)\n",
    "checkpoint_file = checkpoint_dir.joinpath('best_model.weights.h5')\n",
    "\n",
    "with strategy.scope(): \n",
    "    model.load_weights(checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps: Predict ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "threshold = 0.60 \n",
    "\n",
    "def sigmoid(x):\n",
    "\n",
    "    sig =  1 / (1 + math.exp(-x))    \n",
    "    #return 1 if sig > threshold else 0\n",
    "    return 1 if x > threshold else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 213s 2s/step\n"
     ]
    }
   ],
   "source": [
    "#Step 9: Predict the model\n",
    "\n",
    "y_hat = model.predict(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_label index: (array([8], dtype=int64),), label: python\n",
      "Y_predict index : (array([  0,   2,   3,   4,   5,   6,   7,  10,  12,  13,  14,  15,  19,\n",
      "        20,  23,  24,  26,  30,  31,  32,  34,  38,  40,  41,  44,  46,\n",
      "        48,  50,  53,  55,  56,  57,  58,  59,  60,  62,  63,  64,  66,\n",
      "        67,  70,  71,  72,  73,  74,  75,  79,  80,  81,  82,  83,  84,\n",
      "        85,  86,  90,  91,  92,  93,  94,  98,  99, 100, 101, 103, 105,\n",
      "       107, 108, 109, 110, 111, 113, 114, 115, 116, 119, 120, 123, 125,\n",
      "       129, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 144, 145,\n",
      "       146, 147, 148, 150, 151, 153, 154, 156, 157, 159, 162, 163, 164,\n",
      "       165, 167, 168, 169, 174, 175, 176, 178, 179, 181, 183], dtype=int64),), label: php, c#, javascript, android, c++, jquery, iphone, mysql, .net, c, css, sql, ruby-on-rails, database, ruby, algorithm, sql-server, books, regex, html5, wpf, performance, apache, eclipse, image, web-services, php5, oop, visual-studio-2010, programming-languages, application, google, ipad, visual-studio, networking, design, multithreading, ruby-on-rails-3, flash, sql-server-2008, unix, open-source, winforms, forms, frameworks, bash, linq, web-applications, website, web, spring, r, wcf, excel, windows-phone-7, cocoa, delphi, mobile, career-development, function, gui, codeigniter, qt, java-ee, actionscript-3, shell, flex, testing, .htaccess, jsp, class, content-management-system, parsing, swing, google-app-engine, opengl, unit-testing, search, list, seo, pdf, coding-style, rest, debugging, video, cocoa-touch, encryption, google-chrome, sqlite, browser, ios5, entity-framework, url, operating-system, database-design, firefox, sockets, visual-c++, cakephp, optimization, google-maps, table, graphics, hosting, templates, compiler, script, blackberry, iphone-sdk-4.0, ssl, iis, latex, plugins, tsql, memory\n"
     ]
    }
   ],
   "source": [
    "index = 5\n",
    "\n",
    "print(f'Y_label index: {np.nonzero(Y_test[index])}, label: {\", \".join(selected_features_name[np.nonzero(Y_test[index])])}')\n",
    "print(f'Y_predict index : {np.nonzero(list(map(sigmoid, y_hat[index])))}, label: {\", \".join(selected_features_name[np.nonzero(list(map(sigmoid, y_hat[index])))])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f22623e58051c7bd3801d733fbe94b3fa3199d3a48481c264d0b15e70572f7d5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
