COMMON_PARAM:
  TAGS_TO_CHOOSE: 185 #950 # number of tags to consider as y output label. Of 14878 tags:  1. 950: 99.14% of questions 2. 300: 93.22%, 3. 200: 90.36%, 4. 190: 89.99%, 5. 185: 89.811% 
  TRAIN_SPLIT: 0.95  # train valid split size
  MAX_TITLE_LEN: 80 #100 # maximum length of the title text. In our dataset max len post cleaning is 168
  NUM_WORDS: None # number of words to select for tokenization

TRANSFROMER_MODEL:
  BERT_MODEL_NAME: 'bert-large-uncased'
  DENSE_UNITS: 256
  DROP_OUT: 0.2
  LR: 1.00E-05
  BATCH_SIZE: 20 #GPU: 32 #TPU: 64 #19 #12 #64 #32 #16
  EPOCH: 16 #32 #50

LSTM_MODEL:
  LSTM_UNITS: 128
  DROP_OUT: 0.1
  RECURRENT_DROP_OUT: 0.2
  L2_REG: 1e-4
  LR: 0.002
  BATCH_SIZE: 512
  EPOCH: 100
  TRAINABLE_EMBED: True
  MAX_WORD_EMBED_SIZE: 300 #[50, 100, 200, 300]
  WEIGHT_TF_IDF_APPLY: False # Weight vector by term frequency-inverse document frequency 



